import transformers

from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-70b-hf')

tokens = [0, 1, 450, 3014, 310, 10059, 313, 29965, 1451, 9384, 29892, 10059, 29892, 501, 310, 315, 29892, 470, 501, 1451, 29875, 29897, 338, 263, 2024, 5925, 16372, 297, 10059, 29892, 17066, 29892, 3303, 3900, 29889, 450, 16372, 756, 967, 1667, 24165, 297, 10059, 29915, 29879, 9665, 311, 4815, 18403, 29889, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29899, 29871, 29899, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29871, 29899, 29871, 29899, 29871, 29899, 29871, 29899, 29871, 29899, 29871, 29899, 29871, 29899, 29871, 29899, 29871, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899, 29899]
print(tokenizer.decode(tokens))
