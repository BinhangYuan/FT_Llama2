import transformers

from transformers import LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained('meta-llama/Llama-2-70b-hf')

tokens = [0, 1, 450, 3014, 310, 10059, 313, 29965, 1451, 9384, 29892, 10059, 29892, 501, 310, 315, 29892, 470, 501, 1451, 29875, 29897, 338, 263, 2024, 5925, 16372, 297, 10059, 29892, 17066, 29892, 3303, 3900, 29889, 450, 16372, 756, 967, 1667, 24165, 297, 10059, 29915, 29879, 9665, 311, 4815, 18403, 29889, 450, 3014, 310, 10059, 8640, 2246, 29899, 841, 11909, 297, 5164, 4797, 322, 6121, 7115, 886, 29889, 13, 1576, 3014, 310, 10059, 338, 13725, 310, 385, 1090, 5105, 27240, 12755, 29892, 5164, 10591, 403, 11104, 29892, 1006, 2218, 13326, 3821, 844, 5388, 267, 19098, 964, 3023, 21567, 5925, 25884, 322, 9881, 10257, 12462, 29889, 18502, 898, 278, 16930, 322, 21195, 29892, 10059, 338, 884, 1532, 2998, 363, 967, 10257, 12462, 29892, 607, 3160, 278, 349, 18238, 3946, 4523, 310, 27529, 29892, 278, 1952, 720, 4523, 310, 15197, 29892, 278, 7927, 4523, 29892, 278, 4523, 310, 10307, 6692, 23303, 29892, 278, 20349, 4523, 310, 5236, 25219, 16972, 29892, 278, 22196, 4523, 310, 2866, 8675, 292, 25910, 322, 20031, 16972, 322, 278, 4910, 13593, 4523, 29889, 450, 16372, 5279, 427]
print(tokenizer.decode(tokens))
